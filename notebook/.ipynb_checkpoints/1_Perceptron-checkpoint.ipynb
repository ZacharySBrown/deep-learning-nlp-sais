{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "import getpass\n",
    "\n",
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "path = \"/Users/{}/anaconda3/envs/rise_latest/etc/jupyter/nbconfig\".format(getpass.getuser())\n",
    "cm = BaseJSONConfigManager(config_dir=path)\n",
    "o = cm.update(\"livereveal\", {\n",
    "              \"theme\": \"sky\",\n",
    "              \"transition\": \"fade\",\n",
    "              \"start_slideshow_at\": \"selected\",\n",
    "})\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<h1> Deep Learning and Modern Natural Language Processing </h1>\n",
    "<br>\n",
    "Zachary S. Brown\n",
    "<br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "0. NLP Problem Structure\n",
    "1. Text Classification and the Perceptron\n",
    "2. Vectorization and Classification with RNNs\n",
    "3. POS Tagging with RNNs\n",
    "4. Sequence to Sequence Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLP Problem Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General Problem Structure\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/Shape_of_NLP_Problems_0.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binary Document Classification\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/Shape_of_NLP_Problems_1.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-class Document Classification\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/Shape_of_NLP_Problems_2.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-class _Sequence_ Classification\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/Shape_of_NLP_Problems_4.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Starting Easy: Neural Net with Traditional Vectorization\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/Shape_of_NLP_Problems_5.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Classification and the Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topics\n",
    "* The perceptron and neural network optimization\n",
    "* Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Perceptron\n",
    "<center>\n",
    "<img src=\"src/0_Perceptron.png?\" alt=\"perceptron\" style=\"height:400px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weights\n",
    "<center>\n",
    "<img src=\"src/1_Perceptron.png?\" alt=\"perceptron\" style=\"height:400px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forward Pass\n",
    "<center>\n",
    "<img src=\"src/2_Perceptron.png?\" alt=\"perceptron\" style=\"height:400px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss\n",
    "<center>\n",
    "<img src=\"src/3_Perceptron.png?\" alt=\"perceptron\" style=\"height:400px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculate Gradients\n",
    "<center>\n",
    "<img src=\"src/4_Perceptron.png?\" alt=\"perceptron\" style=\"height:400px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Update Weights\n",
    "<center>\n",
    "<img src=\"src/5_Perceptron.png?\" alt=\"perceptron\" style=\"height:400px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perceptron Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid, LogSoftmax\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss, NLLLoss\n",
    "from string import punctuation\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 0. Dataset Loading and Cleaning\n",
    "\n",
    "We'll begin by loading a prepared version of the [Stanford Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), on which we'll train a binary classifier. \n",
    "\n",
    "This dataset contains 50k highly polarized movie reviews from IMDB, labeled with positive or negative sentiment. \n",
    "\n",
    "We'll perform some minimal preprocessing on the text itself, simply case-normalization and removal of punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Not all, but most of this story is Buster bein...</td>\n",
       "      <td>not all but most of this story is buster being...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Eric Bogosian's ability to roll from character...</td>\n",
       "      <td>eric bogosians ability to roll from character ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I am a professional musician who was inspired ...</td>\n",
       "      <td>i am a professional musician who was inspired ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Robin Williams is excellent in this movie and ...</td>\n",
       "      <td>robin williams is excellent in this movie and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>This is a woeful Hollywood remake of a classic...</td>\n",
       "      <td>this is a woeful hollywood remake of a classic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      1  Not all, but most of this story is Buster bein...   \n",
       "1      1  Eric Bogosian's ability to roll from character...   \n",
       "2      1  I am a professional musician who was inspired ...   \n",
       "3      0  Robin Williams is excellent in this movie and ...   \n",
       "4      0  This is a woeful Hollywood remake of a classic...   \n",
       "\n",
       "                                        text_cleaned  \n",
       "0  not all but most of this story is buster being...  \n",
       "1  eric bogosians ability to roll from character ...  \n",
       "2  i am a professional musician who was inspired ...  \n",
       "3  robin williams is excellent in this movie and ...  \n",
       "4  this is a woeful hollywood remake of a classic...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data into a DataFrame\n",
    "data = pd.read_pickle('../data/1a_acl_imdb.pkl')\n",
    "\n",
    "# Definne a simple convenience function for cleaning the strings\n",
    "def clean_text(text):\n",
    "    return \"\".join([c for c in text.lower() if c not in punctuation])\n",
    "\n",
    "# Clean the string labels\n",
    "data['text_cleaned'] = data['text'].map(clean_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 1. Text Vectorization\n",
    "\n",
    "Once we have some (somewhat) clean data, we can then vectorize the corpus the standard term frequency, inverse document frequency. For the sake of time, we'll limit the overall input feature space to the top 1k tokens, based on the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in input corpus: 180395\n"
     ]
    }
   ],
   "source": [
    "# Initialize a TfidfVectorizer Object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit the cleaned text\n",
    "tfidf.fit(data['text_cleaned'])\n",
    "\n",
    "# Examine the total number of tokens in the text\n",
    "print(\"Total tokens in input corpus: {}\".format(len(tfidf.vocabulary_)))\n",
    "\n",
    "# Initialize a TfidfVectorizer Object, this time with a max number of features\n",
    "max_features = 1000\n",
    "tfidf = TfidfVectorizer(max_features=max_features)\n",
    "\n",
    "# Fit the cleaned text\n",
    "features = tfidf.fit_transform(data['text_cleaned']).todense()\n",
    "labels = data.label.values.reshape(-1,1)\n",
    "\n",
    "# Create tuples of the feature/label pairs, \n",
    "# and perform a stratified train/test split\n",
    "all_data = list(zip(features, labels))\n",
    "train_data, test_data = train_test_split(all_data, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 2a. Perceptron Classifier\n",
    "\n",
    "For the simplest perceptron, we'll only need a single linear layer as well as a sigmoid transformation to map the output space from our linear layer into the proper probability distribution. \n",
    "\n",
    "Two other things that need to be considered are the choice of loss funciton and the optimization algorithm. We'll use binary cross entropy for the loss function, and stochastic gradient descent for the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Create a linear single linear layer, \n",
    "# with input shape of our feature space \n",
    "# and output shape of 1 (binary classification)\n",
    "linear = Linear(max_features, 1, bias=True)\n",
    "\n",
    "# Create a instance of the sigmoid function\n",
    "# so we can normalize our output to the range [0,1]\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "# Binary cross entropy is an appropriate loss function \n",
    "# for this type of problem, and is implemented in the \n",
    "# `BCELoss` class in pytroch\n",
    "criterion = BCELoss()\n",
    "\n",
    "# We'll use basic stochastic gradient descent\n",
    "# to optimize the parameters of our linear layer \n",
    "# (the sigmoid is a transformation with no parameters)\n",
    "optim = SGD(params=linear.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature tensor: torch.Size([1, 1000])\n",
      "Shape of linear_output: torch.Size([1, 1])\n",
      "Value of sigmoid_output: tensor([[0.4934]], grad_fn=<SigmoidBackward>)\n",
      "Value of loss: tensor(0.7065, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Bias: tensor([0.0006])\n",
      "Bias gradient: tensor([-0.5066])\n",
      "Bias: tensor([0.0057])\n"
     ]
    }
   ],
   "source": [
    "# let's see what it looks like \n",
    "# to pass a single example through\n",
    "# the objects above\n",
    "\n",
    "# grab a single example\n",
    "f = features[0]\n",
    "t = labels[0]\n",
    "\n",
    "# conver the example to tensors\n",
    "X = torch.FloatTensor(f)\n",
    "y = torch.FloatTensor(t)\n",
    "print(\"Shape of feature tensor:\", X.shape)\n",
    "\n",
    "# pass the input tensor through the linear layer\n",
    "linear_output = linear.forward(X)\n",
    "print(\"Shape of linear_output:\", linear_output.shape)\n",
    "\n",
    "# take the sigmoid of the linear output\n",
    "sigmoid_output = sigmoid(linear_output)\n",
    "print(\"Value of sigmoid_output:\", sigmoid_output)\n",
    "\n",
    "# calculat the loss w.r.t. the expected value\n",
    "loss = criterion(sigmoid_output.view(1,-1), y)\n",
    "print(\"Value of loss:\", loss)\n",
    "\n",
    "# calculate the gradients\n",
    "loss.backward()\n",
    "\n",
    "# check the current value and gradient\n",
    "# of the bias\n",
    "weights, bias = list(linear.parameters())\n",
    "print(\"Bias:\", bias.data)\n",
    "print(\"Bias gradient:\", bias.grad)\n",
    "\n",
    "# take a step with the optimizer\n",
    "# to update the parameters\n",
    "optim.step()\n",
    "\n",
    "# check the value of the bias\n",
    "weights, bias = list(linear.parameters())\n",
    "print(\"Bias:\", bias.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 2b. Training the Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37500/37500 [00:07<00:00, 4962.60it/s]\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "for it, example in tqdm(list(enumerate(train_data))):\n",
    "    \n",
    "    # zero out our gradients for each weight\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    # pull out the features and the target\n",
    "    # for each example\n",
    "    f, t = example\n",
    "    \n",
    "    # cast the feature and target to \n",
    "    # the appropriate torch types\n",
    "    X = torch.FloatTensor(f)\n",
    "    y = torch.FloatTensor(t)\n",
    "    \n",
    "    # start the forward pass\n",
    "    X_prime = linear(X)\n",
    "    output = sigmoid(X_prime)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(output.view(-1), y)\n",
    "    total_loss += loss.data.numpy()\n",
    "    \n",
    "    # calcualte the gradients of the\n",
    "    # loss w.r.t. each of the parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update the weights in the\n",
    "    # linear layer\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 2c. Evaluating the Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.80\n"
     ]
    }
   ],
   "source": [
    "# initialize lists to keep track\n",
    "# of predicted and actual values\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# set a probability threshold \n",
    "# for calculating the accuracy\n",
    "threshold = 0.5\n",
    "\n",
    "# loop through the test examples\n",
    "for f, t in test_data:\n",
    "    X = torch.FloatTensor(f)\n",
    "    y = torch.FloatTensor(t)\n",
    "    output = sigmoid(linear(X))\n",
    "    y_true.append(y.data.numpy()[0])\n",
    "    y_pred.append(output.data.numpy()[0])\n",
    "\n",
    "# calculate a prediction,\n",
    "# then compute the accuracy\n",
    "y_pred = [int(p >= threshold) for p in y_pred]\n",
    "a = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(\"Validation Accuracy: {:.2f}\".format(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 2e. Creating a Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from modules.perceptron import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 2d. Putting it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Loss: 0.61, Validation Accuracy: 0.80\n",
      "Epoch Loss: 0.52, Validation Accuracy: 0.82\n",
      "Epoch Loss: 0.47, Validation Accuracy: 0.83\n",
      "Epoch Loss: 0.44, Validation Accuracy: 0.84\n",
      "Epoch Loss: 0.42, Validation Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "from modules.perceptron import *\n",
    "\n",
    "\n",
    "model = perceptron(max_features)\n",
    "criterion = BCELoss()\n",
    "optim = SGD(params=model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for it, example in enumerate(train_data):\n",
    "            optim.zero_grad()\n",
    "            f, t = example\n",
    "            X = torch.FloatTensor(f)\n",
    "            y = torch.FloatTensor(t)\n",
    "            output = model.forward(X)\n",
    "            loss = criterion(output.view(-1), y)\n",
    "            total_loss += loss.data.numpy()\n",
    "            loss.backward()\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        threshold = 0.5\n",
    "        for f, t in test_data:\n",
    "            X = torch.FloatTensor(f)\n",
    "            y = torch.FloatTensor(t)\n",
    "            output = model.forward(X)\n",
    "            y_true.append(y.data.numpy()[0])\n",
    "            y_pred.append(output.data.numpy()[0])\n",
    "\n",
    "        y_pred = [int(p >= threshold) for p in y_pred]\n",
    "        a = accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "        total_loss /= (it + 1)\n",
    "        print(\"Epoch Loss: {:.2f}, Validation Accuracy: {:.2f}\".format(total_loss, a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 3. Multi-class Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>javascript</td>\n",
       "      <td>Angular.js expression: output transformed obje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>android</td>\n",
       "      <td>How restart Fragment in my Activity - Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>javascript</td>\n",
       "      <td>node.js express, a very strange behaviour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>java</td>\n",
       "      <td>Open Editor Part in E4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>javascript</td>\n",
       "      <td>Prevent Lazy Load of a Div in Laxy Load XT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                               text\n",
       "0  javascript  Angular.js expression: output transformed obje...\n",
       "1     android      How restart Fragment in my Activity - Android\n",
       "2  javascript          node.js express, a very strange behaviour\n",
       "3        java                             Open Editor Part in E4\n",
       "4  javascript         Prevent Lazy Load of a Div in Laxy Load XT"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data into a DataFrame\n",
    "data = pd.read_pickle('../data/1b_stackoverflow_qna.pkl')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in input corpus: 36873\n"
     ]
    }
   ],
   "source": [
    "# Definne a simple convenience function for cleaning the strings\n",
    "def clean_text(text):\n",
    "    return \"\".join([c for c in text.lower() if c not in punctuation])\n",
    "\n",
    "# Clean the string labels\n",
    "data['text_cleaned'] = data['text'].map(clean_text)\n",
    "\n",
    "# Initialize a TfidfVectorizer Object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit the cleaned text\n",
    "tfidf.fit(data['text_cleaned'])\n",
    "\n",
    "# Examine the total number of tokens in the text\n",
    "print(\"Total tokens in input corpus: {}\".format(len(tfidf.vocabulary_)))\n",
    "\n",
    "# Initialize a TfidfVectorizer Object, this time with a max number of features\n",
    "max_features = 1000\n",
    "tfidf = TfidfVectorizer(max_features=max_features)\n",
    "\n",
    "# Fit the cleaned text\n",
    "features = tfidf.fit_transform(data['text_cleaned']).todense()\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(data.label.values).reshape(-1,1)\n",
    "label_size = len(le.classes_)\n",
    "\n",
    "# Create tuples of the feature/label pairs, \n",
    "# and perform a stratified train/test split\n",
    "all_data = list(zip(features, labels))\n",
    "train_data, test_data = train_test_split(all_data, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Create a linear single linear layer, \n",
    "# with input shape of our feature space \n",
    "# and output shape of 1 (binary classification)\n",
    "linear = Linear(max_features, label_size, bias=True)\n",
    "\n",
    "# Create a instance of the sigmoid function\n",
    "# so we can normalize our output to the range [0,1]\n",
    "softmax = LogSoftmax(dim=1)\n",
    "\n",
    "# Binary cross entropy is an appropriate loss function \n",
    "# for this type of problem, and is implemented in the \n",
    "# `BCELoss` class in pytorch\n",
    "criterion = NLLLoss()\n",
    "\n",
    "# We'll use basic stochastic gradient descent\n",
    "# to optimize the parameters of our linear layer \n",
    "# (the sigmoid is a transformation with no parameters)\n",
    "optim = SGD(params=linear.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Tensor Shape: torch.Size([1, 1000])\n",
      "Features Tensor Shape: torch.Size([1])\n",
      "Linear Output Shape: torch.Size([1, 5])\n",
      "Softmax Output Shape: torch.Size([1, 5])\n",
      "Softmax Normalization: tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "Loss: tensor(1.6266)\n"
     ]
    }
   ],
   "source": [
    "# take an example from features and labels\n",
    "f = features[0]\n",
    "t = labels[0]\n",
    "\n",
    "# cast the features and targets\n",
    "# as torch tensors\n",
    "X = torch.FloatTensor(f)\n",
    "y = torch.LongTensor(t)\n",
    "print(\"Features Tensor Shape:\", X.shape)\n",
    "print(\"Features Tensor Shape:\", y.shape)\n",
    "\n",
    "# pass the features tensor through the linear layer\n",
    "linear_output = linear(X)\n",
    "print(\"Linear Output Shape:\", linear_output.shape)\n",
    "\n",
    "# pass the output from the linear\n",
    "# layer through the softmax\n",
    "softmax_output = softmax(linear_output)\n",
    "print(\"Softmax Output Shape:\", softmax_output.shape)\n",
    "\n",
    "# verify that the softmax actually sums to 1\n",
    "# HINT: use torch.exp (this is LogSoftmax) and torch.sum\n",
    "softmax_normalization = torch.exp(softmax_output).sum()\n",
    "print(\"Softmax Normalization:\", softmax_normalization)\n",
    "\n",
    "# calculate the loss\n",
    "loss = criterion(softmax_output, y)\n",
    "print(\"Loss:\", loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.24, Validation Accuracy: 0.63\n",
      "Loss: 1.01, Validation Accuracy: 0.67\n",
      "Loss: 0.93, Validation Accuracy: 0.68\n",
      "Loss: 0.89, Validation Accuracy: 0.69\n",
      "Loss: 0.86, Validation Accuracy: 0.69\n",
      "Loss: 0.84, Validation Accuracy: 0.70\n",
      "Loss: 0.83, Validation Accuracy: 0.70\n",
      "Loss: 0.82, Validation Accuracy: 0.70\n",
      "Loss: 0.81, Validation Accuracy: 0.70\n",
      "Loss: 0.80, Validation Accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "model = multi_class_perceptron(max_features, label_size)\n",
    "optim = SGD(params=model.parameters(), lr=0.01)\n",
    "criterion = NLLLoss()\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    linear.train()\n",
    "    total_loss = 0\n",
    "    for it, example in list(enumerate(train_data)):\n",
    "        optim.zero_grad()\n",
    "        f, t = example\n",
    "        X = torch.FloatTensor(f)\n",
    "        y = torch.LongTensor(t)\n",
    "        output = model.forward(X)\n",
    "        loss = criterion(output, y)\n",
    "        total_loss += loss.data.numpy()\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "        \n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    threshold = 0.5\n",
    "\n",
    "    for f, t in test_data:\n",
    "        X = torch.FloatTensor(f)\n",
    "        y = torch.FloatTensor([t])\n",
    "        output = model.forward(X)\n",
    "        y_true.append(y.data.numpy()[0])\n",
    "        y_pred.append(torch.argmax(output.data).numpy())\n",
    "\n",
    "\n",
    "    a = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    total_loss /= (it + 1)\n",
    "\n",
    "    print(\"Loss: {:.2f}, Validation Accuracy: {:.2f}\".format(total_loss, a))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
